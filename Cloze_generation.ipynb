{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuunnis123/100-Days-Of-ML-Code/blob/master/Cloze_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PvUZA1T-ooIK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fitz\n",
        "! pip install nltk\n",
        "! pip install summa\n",
        "! pip install rake_nltk\n",
        "! pip install PyMuPDF\n",
        "!pip install bert-extractive-summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGXkHQ1HD_pU",
        "outputId": "eb29411e-fe95-417f-92e8-3ae2ee051c17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fitz) (1.7.3)\n",
            "Collecting configobj\n",
            "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from fitz) (0.17.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fitz) (1.3.5)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from fitz) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fitz) (1.21.6)\n",
            "Collecting nipype\n",
            "  Downloading nipype-1.8.4-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 10.4 MB/s \n",
            "\u001b[?25hCollecting pyxnat\n",
            "  Downloading pyxnat-1.4.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj->fitz) (1.15.0)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting rdflib>=5.0.0\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (3.8.0)\n",
            "Collecting looseversion\n",
            "  Downloading looseversion-1.0.1-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (21.3)\n",
            "Collecting prov>=1.5.2\n",
            "  Downloading prov-2.0.0-py3-none-any.whl (421 kB)\n",
            "\u001b[K     |████████████████████████████████| 421 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting simplejson>=3.8.0\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 51.7 MB/s \n",
            "\u001b[?25hCollecting etelemetry>=0.2.0\n",
            "  Downloading etelemetry-0.3.0-py3-none-any.whl (6.3 kB)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (2.6.3)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Collecting traits!=5.0,<6.4,>=4.6\n",
            "  Downloading traits-6.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1 MB 33.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (2.23.0)\n",
            "Collecting ci-info>=0.2\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.7/dist-packages (from prov>=1.5.2->nipype->fitz) (4.9.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot>=1.2.3->nipype->fitz) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 582 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib>=5.0.0->nipype->fitz) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib>=5.0.0->nipype->fitz) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib>=5.0.0->nipype->fitz) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fitz) (2022.2.1)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.7/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from pyxnat->fitz) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->etelemetry>=0.2.0->nipype->fitz) (2022.6.15)\n",
            "Building wheels for collected packages: configobj, pyxnat\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34547 sha256=56cd95e7176ba20f4c55af790cd46c8ced4fd01eb387b04bd09a044f78784cb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n",
            "  Building wheel for pyxnat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyxnat: filename=pyxnat-1.4-py3-none-any.whl size=92687 sha256=07d6c98e60102faac4cc47e85b7c9d2c55d606d0a058f8864fb8564b6b12cffb\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/d3/8f/51847ef95ec6448ff7599bf269145f8290fa82f1ddf00ee90c\n",
            "Successfully built configobj pyxnat\n",
            "Installing collected packages: isodate, rdflib, ci-info, traits, simplejson, prov, looseversion, etelemetry, pyxnat, nipype, configparser, configobj, fitz\n",
            "Successfully installed ci-info-0.3.0 configobj-5.0.6 configparser-5.3.0 etelemetry-0.3.0 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.0.1 nipype-1.8.4 prov-2.0.0 pyxnat-1.4 rdflib-6.2.0 simplejson-3.17.6 traits-6.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54412 sha256=e16b5c5d3634e9f8f360a2d290c16204994fb1cf1d7300a5e5df36d8ca563781\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rake_nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.7/dist-packages (from rake_nltk) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (7.1.2)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.20.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.20.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (1.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (3.4.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.9.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (8.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->bert-extractive-summarizer) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.8.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1 huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qQpugjGsooIL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "c06ae7238053412abfd8a1d417e64935",
            "1e4b69ea0c964010ad44d3fd2b448cd2",
            "e2f479c3e8d74eb98512c4041935144d",
            "9c3011730b814f3294bff5e5114d5ddd",
            "d758c6801be74a46bd2386c3f45e2e1b",
            "302014b078d24e94844b50d68bd0bf98",
            "b028527927cf422b94c1c7a0235194dd",
            "6828169acb854a05822facb5660a90f4",
            "15d842bb7dae441d8837208908669dea",
            "77e5ba96d0904453965850a53b270bf2",
            "1991239b493d4c4eb1a3fa99179c226b"
          ]
        },
        "outputId": "22a7a0a6-62e0-4233-b1b1-fc30ef212c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c06ae7238053412abfd8a1d417e64935"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import fitz\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.porter import *\n",
        "from summarizer import Summarizer\n",
        "from summa import summarizer\n",
        "from rake_nltk import Rake"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_JN6YF6D-qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wvRg4MsPooIM"
      },
      "outputs": [],
      "source": [
        "def is_number(uchar):\n",
        "    if uchar >= u'\\u0030' and uchar<=u'\\u0039':\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def is_alphabet(uchar):\n",
        "    if (uchar >= u'\\u0041' and uchar<=u'\\u005a') or (uchar >= u'\\u0061' and uchar<=u'\\u007a'):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "def is_sign(uchar):\n",
        "    sign_set = {\" \", \"-\", \".\", \"(\", \")\" ,\",\",\":\",\"'\",\"’\"}\n",
        "    if uchar in sign_set:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "def is_char(uchar):\n",
        "    if not (is_alphabet(uchar) or is_number(uchar) or is_sign(uchar)):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "def read_pdf_text(filename):\n",
        "    page_content=[]\n",
        "    text = \"\"\n",
        "    with fitz.open(\"/content/sample_data/\"+filename) as pdf:\n",
        "        page_no = -1\n",
        "        for page in pdf:\n",
        "            blocks = page.get_text('blocks')\n",
        "            blocks = sorted(blocks, key = lambda b: (b[0], b[1]))\n",
        "\n",
        "            for block_word in blocks:\n",
        "                if block_word[4].startswith( '<image:'):\n",
        "                    continue\n",
        "                elif len( block_word[4].replace('\\n','') ) < 10:\n",
        "                    continue\n",
        "                page_content.append(block_word[4])\n",
        "\n",
        "            for page_num in range(len(page_content)):\n",
        "                for j in page_content[page_num]:\n",
        "                    if j == '\\n':\n",
        "                        page_content[page_num] = page_content[page_num].replace(j, ' ')\n",
        "                    if is_char(j):\n",
        "                        page_content[page_num] = page_content[page_num].replace(j, ' ')\n",
        "                page_content[page_num] = \" \".join(page_content[page_num].split())\n",
        "\n",
        "\n",
        "            if(page_no < 0):\n",
        "                for page_num in range(len(page_content)):\n",
        "                    text = text + page_content[page_num] + '. '\n",
        "            else:\n",
        "                text = ''.join(page_content[page_no-1])\n",
        "            \n",
        "    text = text[text.find('Introduction')+12:text.find('References')]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yNBRyQeyooIN"
      },
      "outputs": [],
      "source": [
        "def bert(text):\n",
        "    model = Summarizer()\n",
        "    result = model(text, min_length=60)\n",
        "    summary = ''.join(result)\n",
        "    return summary\n",
        "\n",
        "def get_keyword(summary, text):\n",
        "    # sentence = summarizer.summarize(text_2, ratio=1, split=True)\n",
        "    sentence = sent_tokenize(summary)\n",
        "    sentence = list(dict.fromkeys(sentence))\n",
        "\n",
        "    r = Rake()\n",
        "    summary = summarizer.summarize(text, ratio=0.5)\n",
        "    r.extract_keywords_from_text(summary)\n",
        "    keyword = r.get_ranked_phrases()\n",
        "    keyword = list(dict.fromkeys(keyword))\n",
        "    keyword = list(filter(lambda x: len(x) > 6 and len(x.split()) < 3, keyword))[:30]\n",
        "    return keyword\n",
        "    # random.shuffle(keyword)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!cd sample_data\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur4ih3rDHNCN",
        "outputId": "1163abe9-c849-41b6-958c-f84ea8805423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enRNP3J8ooIN",
        "outputId": "73ed17ec-1590-46a5-87f3-4bf102421d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. Nam et al. (2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). Wise et al. (2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). Dodge et al. (2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. Nam et al. (2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). Wise et al. (2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). Dodge et al. (2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Proactive remediation was provided by teach- ers on topics that stu- dents needed to learn but were not currently struggling with. Field observations of students collected dur- ing class (e.g. engaged disengaged behaviours using the learning system) and log data from the system. Miller et al. (2015) High schools in Texas and West Virginia Distinguish different types of student online and offline learning behav- iours to support teachers’ proactive remediation. The amount of teaching experience may affect teachers’ interpretation of data, and the amount and effectiveness of interventions. Teachers were enabled to attend to groups which needed help, and to address the problem issues. The topics, relevant concepts and progress revealed from the discussion were identi- fied and visualised. Teachers can give sug- gestions to deepen and broaden the discussion. Online chat messages of student groups, cover- ing the information such as topics, relevant concepts, number of words. van Leeuwen et al. (2015) NA Identify collabora- tive writing groups that experience problems and pro- vide suggestions. The effectiveness of the interventions was small for students at a low knowledge level. Students showed a bet- ter long-term reten- tion performance. Provision of personalised retention test schedules based on students’ knowledge levels, and relearning assignments if the students fail in the retention tests. Results of retention tests (based on the percentage of the number of questions answered correctly), and the speed to master relevant skills. Xiong et al. (2015) NA Improve students’ long-term mastery of skills. Students showed a better learning performance after the interventions. Students’ affective states were detected, based on which instructive feedback, problem- solving feedback and reflective prompts were provided. Interaction with the learning platform (e.g. actions completed for learning tasks), students’ perceived task difficulty and the students’ spoken words (collected using speech recognition software). Grawemeyer et al. (2016) NA Provide tailored feedback to students based on their affective state. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. Nam et al. (2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). Wise et al. (2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). Dodge et al. (2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Proactive remediation was provided by teach- ers on topics that stu- dents needed to learn but were not currently struggling with. Field observations of students collected dur- ing class (e.g. engaged disengaged behaviours using the learning system) and log data from the system. Miller et al. (2015) High schools in Texas and West Virginia Distinguish different types of student online and offline learning behav- iours to support teachers’ proactive remediation. The amount of teaching experience may affect teachers’ interpretation of data, and the amount and effectiveness of interventions. Teachers were enabled to attend to groups which needed help, and to address the problem issues. The topics, relevant concepts and progress revealed from the discussion were identi- fied and visualised. Teachers can give sug- gestions to deepen and broaden the discussion. Online chat messages of student groups, cover- ing the information such as topics, relevant concepts, number of words. van Leeuwen et al. (2015) NA Identify collabora- tive writing groups that experience problems and pro- vide suggestions. The effectiveness of the interventions was small for students at a low knowledge level. Students showed a bet- ter long-term reten- tion performance. Provision of personalised retention test schedules based on students’ knowledge levels, and relearning assignments if the students fail in the retention tests. Results of retention tests (based on the percentage of the number of questions answered correctly), and the speed to master relevant skills. Xiong et al. (2015) NA Improve students’ long-term mastery of skills. Students showed a better learning performance after the interventions. Students’ affective states were detected, based on which instructive feedback, problem- solving feedback and reflective prompts were provided. Interaction with the learning platform (e.g. actions completed for learning tasks), students’ perceived task difficulty and the students’ spoken words (collected using speech recognition software). Grawemeyer et al. (2016) NA Provide tailored feedback to students based on their affective state. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Users’ socio-demo- graphic data, their learning goals, their activities within the learning environment and their organisation’s learning requirements. 2016b) Two European organi- sations Improve self- regulated learning through techno- logical scaffolding interventions. Siadaty et al. (2016a,. Self-regulated learning is affected by many factors and scaffolding is only one of them. The interventions were perceived as enhancing users’ self- regulated learning, in terms of areas such as recommending useful information, learning paths and activities. The intervention involved (i) Providing usage infor- mation about available resources (ii) Providing the latest updates on users’ learning goals and resources, as well as colleagues’ learning activities (iii) Showing users’ progress in achieving learning goals (iv) Recommending learning goals to col- leagues by the users (v) Informing users of the learning objectives and requirements of their organisation (vi) Recommending learning paths and learning activities to users (vii) Showing users’ profiles of knowledge sharing. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. Nam et al. (2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). Wise et al. (2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). Dodge et al. (2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Proactive remediation was provided by teach- ers on topics that stu- dents needed to learn but were not currently struggling with. Field observations of students collected dur- ing class (e.g. engaged disengaged behaviours using the learning system) and log data from the system. Miller et al. (2015) High schools in Texas and West Virginia Distinguish different types of student online and offline learning behav- iours to support teachers’ proactive remediation. The amount of teaching experience may affect teachers’ interpretation of data, and the amount and effectiveness of interventions. Teachers were enabled to attend to groups which needed help, and to address the problem issues. The topics, relevant concepts and progress revealed from the discussion were identi- fied and visualised. Teachers can give sug- gestions to deepen and broaden the discussion. Online chat messages of student groups, cover- ing the information such as topics, relevant concepts, number of words. van Leeuwen et al. (2015) NA Identify collabora- tive writing groups that experience problems and pro- vide suggestions. The effectiveness of the interventions was small for students at a low knowledge level. Students showed a bet- ter long-term reten- tion performance. Provision of personalised retention test schedules based on students’ knowledge levels, and relearning assignments if the students fail in the retention tests. Results of retention tests (based on the percentage of the number of questions answered correctly), and the speed to master relevant skills. Xiong et al. (2015) NA Improve students’ long-term mastery of skills. Students showed a better learning performance after the interventions. Students’ affective states were detected, based on which instructive feedback, problem- solving feedback and reflective prompts were provided. Interaction with the learning platform (e.g. actions completed for learning tasks), students’ perceived task difficulty and the students’ spoken words (collected using speech recognition software). Grawemeyer et al. (2016) NA Provide tailored feedback to students based on their affective state. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Users’ socio-demo- graphic data, their learning goals, their activities within the learning environment and their organisation’s learning requirements. 2016b) Two European organi- sations Improve self- regulated learning through techno- logical scaffolding interventions. Siadaty et al. (2016a,. Self-regulated learning is affected by many factors and scaffolding is only one of them. The interventions were perceived as enhancing users’ self- regulated learning, in terms of areas such as recommending useful information, learning paths and activities. The intervention involved (i) Providing usage infor- mation about available resources (ii) Providing the latest updates on users’ learning goals and resources, as well as colleagues’ learning activities (iii) Showing users’ progress in achieving learning goals (iv) Recommending learning goals to col- leagues by the users (v) Informing users of the learning objectives and requirements of their organisation (vi) Recommending learning paths and learning activities to users (vii) Showing users’ profiles of knowledge sharing. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Students’ levels of engagement were enhanced in video watching, discussion and collaborative pro- gramming activities. Their learning out- comes also improved. Email messages were sent and face-to-face discussions were held with students. Log files of the edX LMS, which stored course informa- tion (e.g. number of students and course syllabus) and recorded students’ interactions with the LMS (e.g. video access and discussion posts) and students’ clickstream data collected from a web-based collabora- tive programming environment for meas- uring students’ levels of engagement. Lu et al. (2017) A university in Taiwan Improve students’ learning outcomes and level of engagement. More rigorous and objec- tive evaluation of the effectiveness of inter- ventions is necessary. There was a 7 higher pass rate in the exam for the students receiving interven- tions. Systematic interven- tions took place at various stages of a course, which included welcoming emails, follow-up phone calls, reminders for assign- ment quiz exam on LMS and face-to-face consultation. Students’ demographic data, prior academic results, assignment quiz scores in current courses, degree of interest in the course and in-class feedback collected using a stu- dent response system. Choi et al. (2018) A university in Hong Kong Identify at-risk students and implement proac- tive strategies. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. Nam et al. (2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). Wise et al. (2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). Dodge et al. (2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Proactive remediation was provided by teach- ers on topics that stu- dents needed to learn but were not currently struggling with. Field observations of students collected dur- ing class (e.g. engaged disengaged behaviours using the learning system) and log data from the system. Miller et al. (2015) High schools in Texas and West Virginia Distinguish different types of student online and offline learning behav- iours to support teachers’ proactive remediation. The amount of teaching experience may affect teachers’ interpretation of data, and the amount and effectiveness of interventions. Teachers were enabled to attend to groups which needed help, and to address the problem issues. The topics, relevant concepts and progress revealed from the discussion were identi- fied and visualised. Teachers can give sug- gestions to deepen and broaden the discussion. Online chat messages of student groups, cover- ing the information such as topics, relevant concepts, number of words. van Leeuwen et al. (2015) NA Identify collabora- tive writing groups that experience problems and pro- vide suggestions. The effectiveness of the interventions was small for students at a low knowledge level. Students showed a bet- ter long-term reten- tion performance. Provision of personalised retention test schedules based on students’ knowledge levels, and relearning assignments if the students fail in the retention tests. Results of retention tests (based on the percentage of the number of questions answered correctly), and the speed to master relevant skills. Xiong et al. (2015) NA Improve students’ long-term mastery of skills. Students showed a better learning performance after the interventions. Students’ affective states were detected, based on which instructive feedback, problem- solving feedback and reflective prompts were provided. Interaction with the learning platform (e.g. actions completed for learning tasks), students’ perceived task difficulty and the students’ spoken words (collected using speech recognition software). Grawemeyer et al. (2016) NA Provide tailored feedback to students based on their affective state. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Users’ socio-demo- graphic data, their learning goals, their activities within the learning environment and their organisation’s learning requirements. 2016b) Two European organi- sations Improve self- regulated learning through techno- logical scaffolding interventions. Siadaty et al. (2016a,. Self-regulated learning is affected by many factors and scaffolding is only one of them. The interventions were perceived as enhancing users’ self- regulated learning, in terms of areas such as recommending useful information, learning paths and activities. The intervention involved (i) Providing usage infor- mation about available resources (ii) Providing the latest updates on users’ learning goals and resources, as well as colleagues’ learning activities (iii) Showing users’ progress in achieving learning goals (iv) Recommending learning goals to col- leagues by the users (v) Informing users of the learning objectives and requirements of their organisation (vi) Recommending learning paths and learning activities to users (vii) Showing users’ profiles of knowledge sharing. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Students’ levels of engagement were enhanced in video watching, discussion and collaborative pro- gramming activities. Their learning out- comes also improved. Email messages were sent and face-to-face discussions were held with students. Log files of the edX LMS, which stored course informa- tion (e.g. number of students and course syllabus) and recorded students’ interactions with the LMS (e.g. video access and discussion posts) and students’ clickstream data collected from a web-based collabora- tive programming environment for meas- uring students’ levels of engagement. Lu et al. (2017) A university in Taiwan Improve students’ learning outcomes and level of engagement. More rigorous and objec- tive evaluation of the effectiveness of inter- ventions is necessary. There was a 7 higher pass rate in the exam for the students receiving interven- tions. Systematic interven- tions took place at various stages of a course, which included welcoming emails, follow-up phone calls, reminders for assign- ment quiz exam on LMS and face-to-face consultation. Students’ demographic data, prior academic results, assignment quiz scores in current courses, degree of interest in the course and in-class feedback collected using a stu- dent response system. Choi et al. (2018) A university in Hong Kong Identify at-risk students and implement proac- tive strategies. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The long-term effective- ness of the inter- ventions providing formative feedback to students is not known. Students (especially those classified as high risk) benefitted from the interven- tion in terms of both achievement (i.e. getting a higher course grade) and persistence (i.e. course withdrawal). Students received mes- sages from instructors on their assessment results and feedback (e.g. encouragement and concern) on their level of performance. Scores on an assessment (e.g. quiz, assignment and exam) adminis- tered early in a term and recorded in the LMS. Espinoza and Genna (2018) University of El Paso Identify at-risk students and provide immediate feedback. Email messages were sent which outlined support options, tailored student support programme and teacher enabling course for developing skills for employment. Villano et al. (2018) University of New England Improve student retention Students’ demographic information (e.g. gender and age), insti- tutional variables (e.g. prior studies, study mode and course type), study performance and workload, daily emotion, online activi- ties, etc.. It may be difficult to determine how the results of this particular intervention can be generalised to other learning analytics practices. Students who used the dashboard more frequently tended to get higher course grades. Students’ performance in courses Feedback was provided to students via the dashboard (e.g. current performance, average performance of peers, estimate of students’ final grade and sug- gested resources). Provide feedback to students through dashboard as an intervention tool. van Horne et al. (2018) A research-intensive university in Midwest USA. Billy Tak-ming Wong tamiwong ouhk.edu.hk. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. Educ. https: doi.org 10.1007 s40692-019-00143-7. A review of learning analytics intervention in higher education (2011 2018). Billy Tak ming Wong1 Kam Cheong Li1. Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. Kam Cheong Li kcli ouhk.edu.hk. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Vol.:(0123456789). J. Comput. Educ.. Keywords Learning analytics Intervention Action Higher education. Introduction. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. 2016 Wong et al. 2018). Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. 2017). Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2016 Rienties et al. 2016). Rienties et al. (2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. 2003). They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. J. Comput. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. 2014 Sclater and Bailey 2015). Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. 2016). Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. 2016 Wong and Lavrencic 2016). Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. Choi et al. (2018) summarise the common intervention approaches, viz. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. Rienties et al. (2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2016). Also, for education institutions, Lonn et al. (2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. S nderlund et al. (2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Offering personalised feedback was another frequent type. Table 1 Country region of the institutions practising learning analytics interventions. J. Comput. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. The articles were chosen based on the following criteria:. Overview of the interventions. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. 2. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. 3. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Australia 1. Hong Kong 1. Seven cases did not specify the location of the institutions. Increase study performance 6. Offer personalised feedback to students 5. Improve student retention 4. Help students to make informed academic decisions 3. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Promote collaborative learning 1. Support academic advisors’ just-in-time decision-making 1. Improve student engagement 1. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. An intervention may have more than one type of objective. J. Comput. Educ.. Table 2 Objectives of the learning analytics interventions. Type of objective Frequency. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. J. Comput. Educ.. Table 4 Methods for the learning analytics interventions. Type of data Frequency. Students’ online learning behaviours 14. Students’ study performance 11. Students’ demographics 8. Type of intervention method Frequency. Personalised recommendation 12. Visualisation of learning data 7. Students’ course selection 7. Students’ academic history 3. Students’ perceptions emotions 3. Students’ in-class feedback 2. Students’ study progress 1. Students’ learning goals 1. Teachers’ observations on students 1. Personalised report on study progress performance 5. Personalised assignments assessments 2. Social contact 2. Recommended actions for academic advisors 1. Visualisation of students’ feedback 1. An intervention may use more than one type of data. An intervention may involve more than one type of method. Improved study performance 8. Higher retention registration rate 4. Enhanced productivity effectiveness in learning and teaching 4. Facilitated understanding of progress performance 3. Increased participation engagement in learning 2. Positive perceptions of students 2. Found suitable courses for students 1. Table 5 summarises the outcomes of the interventions. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. For example, the interventions by Wise et al. (2014) and Lu et al. (2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. 2018). As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. J. Comput. Educ.. Table 5 Outcomes of the learning analytics interventions. Type of outcome Frequency. Outcomes of the interventions. Intervention not sustainable at scale 2. Reliance on students’ contribution of data 2. Too many variables and their combinations 2. Difficulty in reaching at-risk students 1. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Difficulty in generalising the results of intervention 1. Distraction for students 1. Lack of benchmarking information 1. Lack of best practices 1. Limited effectiveness for students at a low knowledge level 1. Limited impact of emails 1. Reliance on teachers’ experience 1. Unknown long-term effectiveness 1. An intervention may have more than one type of challenge. J. Comput. Educ.. Table 6 Challenges for the learning analytics interventions. Type of challenge Frequency. Discussion. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. Contrary to the results of Wong et al. (2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. S nderlund et al. (2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. (2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. 2017) has been gradually put into practice. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. 2018). The interventions covered not only dealing with students encountering problems but also enhancing student success in general. For example, the work of McNely et al. (2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. interaction and feedback during writing. Santos et al. (2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. This is also emphasised in the paper by Li et al. (2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. For example, Rivera-Pelayo et al. (2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). This differs from the results of Li et al. (2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Strategies have been proposed in Choi. J. Comput. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. As emphasised by Ga evi et al. (2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. (2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. Educ.. et al. (2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. J. Comput. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. McKay et al. (2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. McNely et al. (2012) A mid-size public research university in Midwestern USA. Smith et al. (2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. Lonn et al. (2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. Rivera-Pelayo et al. (2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). Santos et al. (2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). Cerezo et al. (2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. Nam et al. (2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). Wise et al. (2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). Dodge et al. (2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Proactive remediation was provided by teach- ers on topics that stu- dents needed to learn but were not currently struggling with. Field observations of students collected dur- ing class (e.g. engaged disengaged behaviours using the learning system) and log data from the system. Miller et al. (2015) High schools in Texas and West Virginia Distinguish different types of student online and offline learning behav- iours to support teachers’ proactive remediation. The amount of teaching experience may affect teachers’ interpretation of data, and the amount and effectiveness of interventions. Teachers were enabled to attend to groups which needed help, and to address the problem issues. The topics, relevant concepts and progress revealed from the discussion were identi- fied and visualised. Teachers can give sug- gestions to deepen and broaden the discussion. Online chat messages of student groups, cover- ing the information such as topics, relevant concepts, number of words. van Leeuwen et al. (2015) NA Identify collabora- tive writing groups that experience problems and pro- vide suggestions. The effectiveness of the interventions was small for students at a low knowledge level. Students showed a bet- ter long-term reten- tion performance. Provision of personalised retention test schedules based on students’ knowledge levels, and relearning assignments if the students fail in the retention tests. Results of retention tests (based on the percentage of the number of questions answered correctly), and the speed to master relevant skills. Xiong et al. (2015) NA Improve students’ long-term mastery of skills. Students showed a better learning performance after the interventions. Students’ affective states were detected, based on which instructive feedback, problem- solving feedback and reflective prompts were provided. Interaction with the learning platform (e.g. actions completed for learning tasks), students’ perceived task difficulty and the students’ spoken words (collected using speech recognition software). Grawemeyer et al. (2016) NA Provide tailored feedback to students based on their affective state. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Users’ socio-demo- graphic data, their learning goals, their activities within the learning environment and their organisation’s learning requirements. 2016b) Two European organi- sations Improve self- regulated learning through techno- logical scaffolding interventions. Siadaty et al. (2016a,. Self-regulated learning is affected by many factors and scaffolding is only one of them. The interventions were perceived as enhancing users’ self- regulated learning, in terms of areas such as recommending useful information, learning paths and activities. The intervention involved (i) Providing usage infor- mation about available resources (ii) Providing the latest updates on users’ learning goals and resources, as well as colleagues’ learning activities (iii) Showing users’ progress in achieving learning goals (iv) Recommending learning goals to col- leagues by the users (v) Informing users of the learning objectives and requirements of their organisation (vi) Recommending learning paths and learning activities to users (vii) Showing users’ profiles of knowledge sharing. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Students’ levels of engagement were enhanced in video watching, discussion and collaborative pro- gramming activities. Their learning out- comes also improved. Email messages were sent and face-to-face discussions were held with students. Log files of the edX LMS, which stored course informa- tion (e.g. number of students and course syllabus) and recorded students’ interactions with the LMS (e.g. video access and discussion posts) and students’ clickstream data collected from a web-based collabora- tive programming environment for meas- uring students’ levels of engagement. Lu et al. (2017) A university in Taiwan Improve students’ learning outcomes and level of engagement. More rigorous and objec- tive evaluation of the effectiveness of inter- ventions is necessary. There was a 7 higher pass rate in the exam for the students receiving interven- tions. Systematic interven- tions took place at various stages of a course, which included welcoming emails, follow-up phone calls, reminders for assign- ment quiz exam on LMS and face-to-face consultation. Students’ demographic data, prior academic results, assignment quiz scores in current courses, degree of interest in the course and in-class feedback collected using a stu- dent response system. Choi et al. (2018) A university in Hong Kong Identify at-risk students and implement proac- tive strategies. Table 7 (continued). J. Comput. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. The long-term effective- ness of the inter- ventions providing formative feedback to students is not known. Students (especially those classified as high risk) benefitted from the interven- tion in terms of both achievement (i.e. getting a higher course grade) and persistence (i.e. course withdrawal). Students received mes- sages from instructors on their assessment results and feedback (e.g. encouragement and concern) on their level of performance. Scores on an assessment (e.g. quiz, assignment and exam) adminis- tered early in a term and recorded in the LMS. Espinoza and Genna (2018) University of El Paso Identify at-risk students and provide immediate feedback. Email messages were sent which outlined support options, tailored student support programme and teacher enabling course for developing skills for employment. Villano et al. (2018) University of New England Improve student retention Students’ demographic information (e.g. gender and age), insti- tutional variables (e.g. prior studies, study mode and course type), study performance and workload, daily emotion, online activi- ties, etc.. It may be difficult to determine how the results of this particular intervention can be generalised to other learning analytics practices. Students who used the dashboard more frequently tended to get higher course grades. Students’ performance in courses Feedback was provided to students via the dashboard (e.g. current performance, average performance of peers, estimate of students’ final grade and sug- gested resources). Provide feedback to students through dashboard as an intervention tool. van Horne et al. (2018) A research-intensive university in Midwest USA. Beattie, S., Woodley, C., Souter, K. (2014). Creepy analytics and learner data rights: Proceedings of the Ascilite 2014 Conference (pp. 421 425). Dunedin, New Zealand. Bramucci, R., Gaston, J. (2012). Sherpa: Increasing student success with a recommendation engine: Proceedings of the 2nd International Conference on Learning Analytics and Knowledge LAK’12 (pp. 82 83). Vancouver, Canada. Cerezo, R., Suarez, N., N ez, J. C., S nchez-Santill n, M. (2014). eGraph tool: Graphing the learn- ing process in LMSs: Proceedings of the 4th International Conference on Learning Analytics and Knowledge LAK’14 (pp. 273 274). Indianapolis, Indiana, USA. Choi, S. P. M., Lam, S. S., Li, K. C., Wong, B. T. M. (2018). Learning analytics at low cost: At-risk student prediction with clicker data and systematic proactive interventions. Educational Technology Society, 21(2), 273 290. Clow, D. (2012). The learning analytics cycle: Closing the loop effectively: Proceedings of the 2nd Inter- national Conference on Learning Analytics and Knowledge (pp. 134 138). Vancouver, Canada. Clow, D. (2013). An overview of learning analytics. Teaching in Higher Education, 18(6), 683 695. Corrin, L., Kennedy, G., de Barba, P. G., Lockyer, L., Ga evic, D., Williams, D., Bakharia, A. (2016). Completing the loop: Returning meaningful learning analytic data to teachers. Retrieved from http: melbo urne-cshe.unime lb.edu.au data asset s pdffi le 0006 20839 38 Loop Handb ook.pdf. Dodge, B., Whitmer, J., Frazee, J. P. (2015). Improving undergraduate student achievement in large blended courses through data-driven interventions: Proceedings of the Fifth International Confer- ence on Learning Analytics and Knowledge (pp. 412 413). New York, USA. Espinoza, P., Genna, G. M. (2018). Hi, I want to talk to you about your progress: A large course intervention for at-risk college students. Journal of College Student Retention: Research, Theory Practice. https : doi.org 10.1177 15210 25118 79005 4. Fuchs, D., Mock, D., Morgan, P., Young, C. (2003). Responsiveness-to-intervention: Definitions, evi- dence, and implications for the learning disabilities construct. Learning Disabilities Research and Practice, 18(3), 157 171. Ga evi , D., Dawson, S., Pardo, A. (2017). How do we start State and directions of learning analyt- ics adoption. International Council for Open and Distance Education. Retrieved from https : icde. membe rclic ks.net asset s RESOU RCES draga n la repor t 20cc 20lic ence.pdf. Ga evi , D., Dawson, S., Rogers, T., Ga evi , D. (2016). Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success. The Internet and Higher Education, 28, 68 84. Grann, J., Bushway, D. (2014). Competency map: Visualizing student learning to promote student success: Proceedings of the 4th International Conference on Learning Analytics and Knowledge LAK’14 (pp. 267 268). Indianapolis, USA. Grawemeyer, B., Mavrikis, M., Holmes, W., Gutierrez-Santos, S., Wiedmann, M., Rummel, N. (2016). Affecting off-task behaviour: How affect-aware feedback can improve student learning: Proceedings of the 5th International Conference on Learning Analytics and Knowledge LAK’16 (pp. 104 113). Edinburgh, United Kingdom. Jayaprakash, S. M., Laur a, E. J. M. (2014). Open academic early alert system: Technical demon- stration: Proceedings of the 4th International Conference on Learning Analytics and Knowledge LAK’14 (pp. 267 268). Indianapolis, USA. Khalil, M., Ebner, M. (2015). Learning analytics: Principles and constraints: Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications 2015 (pp. 1326 1336). Montr al, Canada. Kimberly, E. A., Pistilli, M. D. (2012). Course signals at Purdue: Using learning analytics to increase student success. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge LAK’12 (pp. 267 270). Vancouver, Canada. Li, K. C., Ye, C. J., Wong, B. T. M. (2018). Status of learning analytics in Asia: Perspectives of higher education stakeholders. In Lam, et al. (Eds.), Technology in education: Innovative solutions and practices (pp. 267 275). New York: Springer. Lonn, S., Aguilar, S., Teasley, S. D. (2013). Issues, challenges, and lessons learned when scaling up a learning analytics intervention: Proceedings of the 3rd International Conference on Learning Ana- lytics and Knowledge LAK’13 (pp. 235 239). Leuven, Belgium.. J. Comput. Educ.. \n"
          ]
        }
      ],
      "source": [
        "filename = '0_A review of learning analytics intervention in higher education (2011–2018).pdf'\n",
        "for file in glob.glob('/content/sample_data' + filename):\n",
        "    head, filename = os.path.split(file)\n",
        "ks_list = pd.DataFrame(columns = ['sentence', 'key_phrase'])\n",
        "\n",
        "text = read_pdf_text(filename)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": false,
        "id": "RTY2wU8UooIO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "7dede63bfa8840f48bc2f1dcc4e2b337",
            "3e6caa652c7b44d2ac0d92a91f4be00e",
            "faed98f719674c5f896c1f09c64d0c34",
            "ba2fd43d0a2a4341a7aa8df4214f67ee",
            "178c814fa2c74992837dd3c05c773df5",
            "cb5d0d68094648efa6d394c9c23d429c",
            "fc1f98b368774a7c83fbf9fcf5dacc4b",
            "d37984fcb312493d8b0c6ff5cd5ecdcf",
            "c71ab8470d86488b9bfc097195247a91",
            "bdb006a0df994b45b4e28fac6d0af903",
            "124c0f85adb14a6b85acc9a965d2f2b9",
            "f6bec2e38f6247f5a1f9e0a46d3f2fd7",
            "e35c9cec20c448ae9e9622e4dfa9e635",
            "d6ec2d2370ec4143a9a07708c95ade68",
            "e290dfeef01b42928c45303215149ca5",
            "c7868ca9b66c40f2a091db6db705b41b",
            "01aade197295492fa7f8533bcdcf85c6",
            "987fbc078dee4aefaf4e7f31ddff93a2",
            "30b870f983704221921269a9c6170bce",
            "d0f540538d5b48bc98f01437b8154fc8",
            "4f72900689c14b4b8fb2369b4ae6dc9b",
            "906cd4aae8cf4f00bd2baac8ff24c0fc",
            "bf11426f0de5464880b3151f04d052db",
            "4ca245fae6954482b1dacddde060388f",
            "8e88fa1181fd405dae9299bd78d34225",
            "82ec2b5a84f347af9a322c39ae13b13e",
            "58de8685cb5d4bd4b7e7bc57ca74fb5f",
            "6df8cb36dc3f4132b9007b4e583a6545",
            "48f990e5df9a47c4887fd7b9fcd25cc0",
            "b9ced279e45b4464a159d0890ddf64ef",
            "d4f7ee4965e54df2a94ce40c172ae36f",
            "2648beb1459e47a4b4f061b230c7221c",
            "afe472a57f364c85bec98539f81cc022",
            "9f5d3b7ddb3042978603a821d65c3098",
            "9d85cb345b07449bbe1b31b0d815dfb4",
            "7890e1828ae047609599334d0dad7186",
            "9bf215929dbe4fbbab3eb377d28cba80",
            "e59b89da4a364e818ebb614eec15be3a",
            "112e540969c1489e88cc05ad57d7a072",
            "5f52427017784b45b94c1ca508823538",
            "e4f8fe63eb5248f8bdfa8f165fb1b1c2",
            "715632eeb59b4e038e7b6fb8354f52ad",
            "5fac1809797b4ce0aeae44c9db9445c0",
            "0c17f4eda968422cba165cfeebf13a74"
          ]
        },
        "outputId": "472b3cbf-6173-4f48-d812-f6cddf84e143"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dede63bfa8840f48bc2f1dcc4e2b337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6bec2e38f6247f5a1f9e0a46d3f2fd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf11426f0de5464880b3151f04d052db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5d3b7ddb3042978603a821d65c3098"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (277) found smaller than n_clusters (586). Possibly due to duplicate points in X.\n",
            "  model = self._get_model(k).fit(self.features)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). One major objective of learning analytics is to discover early those students who are likely to fail in their studies and provide them with just-in-time and personalised support (Sclater et al. Khalil and Ebner (2015) proposed a life cycle of learning analytics which consisted of four stages: (1) data generation usually from learning platforms, such as massive open online courses, learning man- agement systems (LMS) and virtual learning environments (2) tracking traceable data of learners on the learning platforms (3) analysis to generate patterns and retrieve information from the data and (4) action which may include prediction, intervention and personalisation. As the final stage of the learning analytics cycle, intervention has been claimed to be the biggest challenge in learning analytics (Rienties et al. Although the idea of making learning analytics data meaningful and available to learners has been promoted in recent years, most research still puts an emphasis on developing an accurate prediction model and few studies have addressed the intervention strategies (Clow 2012 Corrin et al. 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. This paper aims to review the case studies of learning analytics intervention in higher education. Within the limited availability of empirical studies on this topic, it provides a categorisation of the intervention methods and the data used for analysis, and identifies the outcomes and challenges as shown in the intervention practices. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Within the scope of academic or instructional areas, interventions refer to the provi- sion of assistance to learners who are at risk or underachieving when compared to an existing standard (Fuchs et al. They aim to prevent learners’ academic failure by monitoring their progress, providing additional instruction or support that matches learners’ needs and influencing their physical, intellectual and moral development. With the support of learning analytics, interventions usually target improving unde- sirable situations such as a dropping retention rate or an unsatisfactory pass rate, which. Abstract Intervention has long been practised in higher education to provide assis- tance for at-risk or underachieving learners. With the development of learning ana- lytics, the delivery of intervention has been informed by data-driven approaches to identify learners’ problems and provide them with just-in-time and personalised sup- port. However, intervention has been claimed to be the greatest challenge in learning analytics and has yet to be widely implemented. This paper reviews 24 case studies of learning analytics intervention in higher education. The cases were categorised and summarised according to their objectives, the data used, the intervention meth- ods, the outcomes obtained and the challenges encountered. The results show that intervention practices have focused most frequently on increasing students’ study performance, offering personalised feedback and improving student retention. The frequent types of data involved students’ online learning behaviours, study perfor- mance, demographics and course selection information. The most commonly used intervention methods involved offering personalised recommendations and visualis- ing learning data. The interventions have led to outcomes such as enhancing study performance, retention and course registration, as well as productivity and effec- tiveness in learning and teaching. The challenges covered a wide range of aspects, including the scalability of intervention, conditions for implementing intervention, limitations of the channels for delivering intervention and the evaluation of interven- tion effectiveness. The results suggest that learning analytics intervention has the potential to further extend its scope of practices to serve a wider range of purposes, but more studies on the empirical evidence, even with null or negative results, are needed to support its long-term effectiveness and sustainability.. J. Comput. A review of learning analytics intervention in higher education (2011 2018). Received: 26 October 2018 Revised: 23 April 2019 Accepted: 14 May 2019 Beijing Normal University 2019. 1 The Open University of Hong Kong, Ho Man Tin, Hong Kong, China. Educ.. Keywords Learning analytics Intervention Action Higher education. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Educ.. are informed by data on the learning progress of students. Their potential advantages have been recognised in literature. For example, students who are less likely to attain the learning objectives of a course can obtain support to improve their performance (Beattie et al. Instructors can take the opportunity to consolidate their relationship with students, reflect on their teaching performance and adjust the teaching or course contents accordingly (Rienties et al. Also, faculty or school administrators can enhance course modules or make better use of resources and information technology infrastructure through resource re-allocation and collabo- ration across departments (Molinaro et al. Learning analytics interventions can take different forms, ranging from vari- ous kinds of communication, such as emails or phone calls, to the imposition of additional assessment. emails, phone calls, instant messaging, posts and news on LMS, group consultation, face-to-face consultation, video recording, peer review and e-tutorials. However, these involve different costs for instance, for instructors, face-to-face consultation is much more costly than emails, although it may be a more effective approach. 2017) argue that the greatest challenge is the uncertain impact of different types of intervention on learners’ attitude, behaviour and cognition. It has been found that students and instructors may also find intervention challenging. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Instructors face the problem of the cost of time, espe- cially those who have larger courses, as insufficient time and inadequate availabil- ity of resources may restrict them from taking proactive intervention (Corrin et al. 2015) found that the main chal- lenge of scaling up a learning analytics intervention comes from technological barri- ers. These challenges are one factor which has led to the limited practice of learning analytics intervention reported so far. The limited practice of learning analytics interventions has been shown in previ- ous reviews. For example, Si Na and Tasir (2017) reviewed the learning analytics interventions in 2012 2016 and found 13 relevant articles, among which only six reported empirical intervention practices. 2018) found that only 11 articles on learning analytics interventions evaluated their effectiveness, which was generally limited by factors such as simple evaluation designs, convenience sampling and small study populations. Also, Sclater (2017) described interventions as an emerging area, while relatively undocumented, in learning analytics. In view of the limited coverage of previous reviews, the current status of learning analytics interventions has yet to be fully revealed and a wider review is needed.. This paper reviews the case studies on learning analytics intervention. Case studies reporting learning analytics intervention were first collected via Scopus and the Web of Science using the key terms ( learning analytics AND intervention ) for the period. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. The 24 case studies of learning analytics interventions are summarised in the Appendix. In terms of geographical locations, Table 1 shows the country region of the institutions where the interventions were practised. Just over half the interven- tions took place in the United States, with the others being in Australia, Hong Kong, Taiwan and Spain together with seven cases which did not specify the locations.. Objectives of the interventions. Table 2 summarises the objectives of the learning analytics interventions. The most frequent type of objective was related to improving students’ study performance, covering remedial actions taken to help those who were at risk of failure or encoun- tering learning difficulties. Table 1 Country region of the institutions practising learning analytics interventions. Educ.. 2011 2018.1 In addition, more relevant articles were collected from the proceedings of the International Learning Analytics Knowledge Conferences for the same period of time. 1 Relevant articles were found on Scopus and the Web of Science starting from 2011.. 1. They reported at least one empirical case of intervention in a higher education institution or an online education platform. They contained the rationale for the intervention strategy, a description of the practice and the outcomes. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. The details of the intervention practices, including the institution, objective, data used, intervention method, outcome and challenge, were then identified and summarised.. Country region Frequency. Seven cases did not specify the location of the institutions. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. Seven cases did not specify the location of the institutions. Enhance students’ self-awareness self-reflection self-regulation 3. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. The data related to students’ online learning behaviours were most frequently used, which usually involved students’ actions on LMS or platforms specifically developed for learning analytics (e.g. login, access to materials, participation in discussion and col- laborative activities). This was followed by students’ study performance (e.g. assign- ment exam scores, course grades and GPA), demographic information (e.g. gender, age, ethnicity and socio-economic status) and course selection. Some types of data were used relatively infrequently, including students’ academic history (e.g. prior studies and study modes), perceptions emotions (e.g. perceived task difficulty, inter- est in a course and daily emotion) and in-class feedback (e.g. votes and qualitative responses). There were also a few types of data which were used only in one practice, including students’ study progress, learning goals and teachers’ observations of stu- dents, where manual labelling and coding may be required to process the data.. Intervention methods. Table 4 shows the intervention methods applied in the case studies. Offering per- sonalised recommendations was the most frequent method, with the areas covered including course selection, online in-person services, learning resources and tailored programmes. The recommendations were delivered through various channels such. Educ.. Table 2 Objectives of the learning analytics interventions. of objective, where students’ specific profiles were often analysed or categorised for tailoring feedback to cope with their needs. Other types of objectives focused on the student perspective, which included improving student retention helping students to make academic decisions such as course selection enhancing their self-awareness, self-reflection and self-regulation promoting collaborative learning and improving student engagement. In addition, some interventions aimed to support the work of staff, such as tracking students’ learning process and performance, and helping them to make just-in-time decisions.. Data collected for the interventions. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. as emails, SMS, voice messages, social media and face-to-face meetings. Visuali- sation of learning data was the second most frequent type of method, with diverse applications such as helping students to get a better understanding of their own or peers’ learning behaviours in order to adjust learning strategies facilitating teach- ers to easily interpret student learning progress for determining remedial actions and highlighting key points made in online discussion for teachers to deepen and broaden the discussion. The third most frequent type of method was providing per- sonalised reports through emails or the dashboard on students’ study progress or performance, based on which students or teachers could take timely action on the situations. Several intervention methods were used only in one or two cases. For students, personalised assignments or assessments were provided based on students’ knowl- edge levels or development needs for specific skills, and informal social contact was made with students such as welcoming emails, phone calls and reminder emails for assignments and quizzes. For teachers academic advisors, recommended actions were shown for their reference (e.g. encouraging students to keep doing well, explor- ing their progress in detail and engaging them in accessing possible difficulties), and students’ in-class live feedback was visualised for the teachers to evaluate students’ knowledge or perceptions and make adjustments in teaching.. Table 3 Data used for the learning analytics interventions. Educ.. Table 4 Methods for the learning analytics interventions. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Improvement in students’ study performance (e.g. course grades, pass rate in exams and mastery of knowl- edge) was most frequently reported. This was followed by a higher retention regis- tration rate higher productivity effectiveness in learning and teaching (in areas such as students’ self-regulation, collaborative learning and teachers’ readiness to react to students’ situations) and better and easier understanding of study progress and per- formance (e.g. assessment of students’ competency against teachers’ expectations and identification of students who needed help). There were also a few types of outcomes which showed in one or two cases only. 2017) helped students to monitor, reflect on and participate in learning activities such as online discussion. Students’ perceptions of learning analytics were also improved, which encouraged them to make use of the analytics results (Kimberly and Pistilli 2012). Students were also helped to find alternative and suitable courses when their pre- ferred courses were full (Bramucci and Gaston 2012).. Challenges encountered. Table 6 shows the challenges reported in the case studies regarding the intervention practices. It is notable that the challenges covered a wide range of aspects where each type involved at most only two cases. In general, there were challenges in the scal- ability of intervention (e.g. too many requests for help from students, and complexity of variable combinations) conditions for implementing the interventions (e.g. stu- dents’ contribution of data, teachers’ experience, being able to reach at-risk students and coordination of variable groups of professionals) limitations of the channels for interventions (e.g. email and visualisation) as well as evaluation of intervention effectiveness (e.g. difficulties in evaluation and generalisation of intervention results). These challenges are possible factors leading to the limited number of empiri- cally tested learning analytics programmes (S nderlund et al. As a conse- quence, some other challenges were encountered in the intervention practices, such as a lack of benchmarking of information and best practices of learning analytics interventions, and uncertainty about the long-term effectiveness of the interventions.. An intervention may have more than one type of outcome. Educ.. Table 5 Outcomes of the learning analytics interventions. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Difficulty in coordinating different groups of professionals working together 1. Difficulty in evaluating the effectiveness of intervention 1. Limited effectiveness for students at a low knowledge level 1. Educ.. Table 6 Challenges for the learning analytics interventions. This paper has presented an overview of learning analytics interventions, covering the types of data used, methods applied, outcomes obtained and challenges encountered. The results contribute to addressing a gap in literature as existing intervention practices have yet to be comprehensively reviewed. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. In terms of the geographical locations, the results of this review show that the learn- ing analytics interventions were mainly implemented in the USA. 2018) that learning analytics has in general been practised in an increasingly wider range of countries, the findings of this review suggest that at least from the published literature learning analytics interventions have yet to be widely practised in various parts of the globe. 2018) suggest that the actual use of learning analytics interventions might be higher than is reported in the literature, with some not published due to null effects or being seen as commercially sensitive. Nonetheless, the 24 intervention practices found in literature and included in this review exceed that in previous reviews i.e. 13 in Si Na and Tasir (2017) and 11 in S nderlund et al. ( 2018) , showing that intervention while being seen as the biggest challenge in learning analytics (Rienties et al. Among the intervention practices reviewed, their objectives were broader than those usually stated in literature which focus on providing remedial actions for students underperformed or at risk of dropping out (Si Na and Tasir 2017 S nderlund et al. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2012) aimed to promote collaborative learning for students participating in online col- laborative writing activities by facilitating their formative assessment of peer-to-peer. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. They illustrated how the intervention strategy was informed by data analysis in learning analytics.. Following these criteria, a total of 24 practices of learning analytics were selected. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2013) enhanced students’ self- awareness, self-reflection and sense-making in learning through deepening their under- standing of their own and peers’ learning behaviours. In this sense, future practices of learning analytics could be devised with a more proactive approach. Rather than early detection or prediction of students’ problems for remediation, aiming to improve stu- dents’ effectiveness in learning through personalised support may benefit a wider range of students and reduce the occurrence of learning problems at the beginning. The data used for the interventions covered both online and face-to-face learning contexts. In addition to the types of data which usually existed in an institution’s LMS VLE SIS (e.g. students’ demographics, online learning behaviours and study performance), there were also types of data collected specifically for the learning analytics practices (e.g. students’ perceptions, emotions and learning goals). The current state of data collection for intervention shows that a major issue in learning analytics lies in sourcing the required data (Ga evi et al. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. However, some types of data (e.g. teachers’ observations of students) were shown to be more costly in terms of data collection and processing. To allow learning analytics inter- vention to be sustainable at scale one of the major challenges reported in the stud- ies reviewed more cost-effective ways to source the data which do not commonly exist in an institution’s data warehouse for learning analytics should be explored. The various types of intervention methods covered visualising learning data, offering personalised reports, recommendations, assignments or assessments and social contact which students or teachers could react to and take corresponding actions. The methods in general involved a change in student behaviours in order to have an effect. It is thus important to give enough time and information for students to reflect on and take action on the feedback provided. 2018), which surveyed the views of academics and administra- tors in higher education institutions on the areas that should be addressed in learn- ing analytics practices. In addition, it is notable that the interventions did not at all follow only a pathway to enhance student success and retention. There were also a few practices which targeted improving teaching. 2013) gathered and visualised students’ in-class live feedback for lecturers to evalu- ate students’ knowledge or perceptions and make adjustments in teaching. While existing learning analytics interventions have focused mainly on student learning, the areas other than learning, such as teaching and course administration, have been rarely addressed and therefore should receive more attention. The challenges faced by interventions were mainly encountered during the imple- mentation process (e.g. difficulties in reaching at-risk students and evaluating effec- tiveness). 2018) which showed the obstacles for some institutions in starting to implement learning analytics, such as the lack of institutional support and a negative perception of institutional management on learning analytics. However, the current review has shown that many of these obsta- cles had been resolved in existing intervention practices. As regards the challenge on sustaining intervention at scale, the problems mainly lie in the limited time of instructors for handling students’ problems. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. Educ.. This review study shows that learning analytics interventions have enabled students’ learning to be enhanced, their problems to be identified early and timely personal- ised support to be offered. The cases of interventions reviewed also suggest that they have the potential to further extend the scope of practices to serve a wider range of purposes. They demonstrate how the data available in institutions can be utilised to support various kinds of learning analytics practices, which could be applica- ble to diverse educational contexts ranging from face-to-face to blended and online learning. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. While the case studies revealed a lack of empirical evidence to support the long-term effectiveness of learning analytics interventions, more studies on empirical experience, even with null or negative results, should be reported to serve as examples for future intervention practices. The cases of intervention highlighted the importance of personalised feedback to address students’ specific problems: as Ga evi et al. ( 2016) stressed, learning analyt- ics should not aim to be a one-size-fits-all solution but interventions should be geared to students’ particular situations and personal needs. It is noted that, among the lim- ited studies on learning analytics interventions, very few have reported quantitative analyses data (Wong 2017). As the effectiveness of interventions may vary among groups in different settings, future research should address the contextual diversity. The practice of intervention, as the final stage of the learning analytics cycle (Khalil and Ebner 2015), relies on the input from the previous stages for identifying specific problems to be tackled. The choice of intervention methods also depends on factors such as the capacity of the infrastructure (e.g. the availability of a data visu- alisation function on LMS) and human resources, as well as the nature and urgency of the problems. Future work should therefore also examine the interrelationships between the various factors to identify how effective intervention methods can be devised.. Acknowledgements The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC IDS16 15).. J. Comput. 2018) for using different intervention methods according to the significance and urgency of student problems, so that less costly but scalable methods (e.g. email reminders) can be used for students without significant difficulties, while methods which are costly and not scalable (e.g. face-to-face consultation) are used only for students in need.. Conclusion. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Educ.. See Table 7.. Table 7 Summary of learning analytics interventions. Source Institution Objective Data Intervention method Outcome Challenge. The system helped students who were closed out of a class to find a suitable alternative class and it was integrated with the learning manage- ment system to provide personalised announcements. Personalised recommen- dations were provided covering course selec- tion, study information and online services, in multimodal formats (e.g. email, SMS, voice message or social media) and based on various triggers (e.g. time, event or location). Student profiles as data source rules created by subject matter experts to determine the condi- tions for sending out recommendations. Provide personalised recommendations to assist students in making better- informed academic decisions. Bramucci and Gaston (2012) South Orange County Community College District. There were too many emails from concerned students to faculty seeking help and there was a lack of best practices in using the system. There was about a 10 point increase in Grade A B by students, and a significantly higher retention rate. Positive perceptions were given by both students and faculty members on the system. Personalised emails as well as traffic signals’ indicating how each student is doing were sent to students by faculty members. Kimberly and Pistilli (2012) Purdue University Increase student success Students’ grades students’ demographic characteristics and past academic history students’ actions on LMS. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. Educ.. Source Institution Objective Data Intervention method Outcome Challenge. Personalised advice was delivered to students, covering their study habits, assignments for practice, feedback on progress, predictions for final grade and encouragement for them. Information about stu- dents’ progress in the course concerned, and performance in other courses advice from students and faculty members. 2012) University of Michigan Offer personalised academic coaching for students. Learners’ individual contributions to col- laborative writing work can be captured only when they produced part of the writing. Using the system, participants col- laborated on complex knowledge work projects in creative and productive ways. Visualisations were generated from docu- ment revision history to help users gain a better sense of ongoing text development as a collaborative writing activity. Data on collaborative writing activities recorded by Google Docs. Foster students’ metacognition and promote collabora- tive learning. 2012) A mid-size public research university in Midwestern USA. 2012) Rio Salado College Improve students’ retention Data on online student activities (e.g. logged into course selec- tion page, viewed assessment feedback and opened a lesson), assignment grades and enrolments, which were collected from LMS and SIS. Interventions designed by faculty members did not generate significant improvements in suc- cess rates, possibly because of the difficulty for instructors to reach students by phone calls. There was a 40 decrease in dropout rate compared to the control group. Students were labelled as Low, Moderate and High according to their warning levels identi- fied by the predictive models. Direct and informal contact (e.g. phone calls) was made and welcome emails were sent to the students. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. NA Technical, cultural and process-oriented chal- lenges may be unavoid- able as different groups of professionals work together. Recommended actions were shown for advisors’ refer- ence, including encourage’ students to keep doing well, explore’ students’ progress in more detail or engage’ students immediately to assess possible academic dif- ficulties. Grade information, student demographic and course history information stored in the LMS. 2013) A university in USA Provide early warn- ing on student academic perfor- mance to support academic advisors’ just-in-time decision-making. There was a potential distraction for students concentrating on the app instead of the class, and there were concerns regarding the voluntarily participa- tion in giving feedback. The effectiveness relied on motivating stu- dents to use the app and the readiness of lecturers to react on feedback retrospec- tively. User-input feedback such as vote Students’ live feedback was openly gath- ered, aggregated and visualised, for lecturers to evaluate students’ knowledge or percep- tions and make timely adjustments. 2013) NA Provide live feed- back from students to lecturers, and improve the students’ attitude, attention and con- centration during classes. Students may not know whether they were doing well or needed to change their learning behaviours, even when the learning traces were given. The tool had a poten- tially higher impact for students working in groups on the same topic than for students working individually on differ- ent topics. Different learning traces were visualised for stu- dents’ self-awareness and reflection, and understanding about peer behaviours. Learning traces of students, such as time spent on a course, resource use (e.g. wiki and blog) and social media use (e.g. Twitter). 2013) NA Enhance students’ self-awareness, self-reflection and sense-making through the use of a learning analyt- ics dashboard. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Log files of LMSs (e.g. content view, task sub- mission, forum view and participation). 2014) University of Oviedo Improve the effectiveness of tracking students’ learning process in LMSs. The use of the tool helped students to understand their com- petency performance in relation to the faculty’s expecta- tions, and improved the course registra- tion rate. Students’ performance levels in relation to specific competencies were visualised using various colours. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Grann and Bushway (2014) Capella University Visually indicate a student’s performance level relative to specific competencies. Students at academic risk were identified based on a predictive model developed using the data, for faculty mem- bers to follow with suitable intervention strategies. Data from SIS (i.e. stu- dent demographic and aptitude data, course grades and course related data) and LMS (i.e. student interac- tion data generated in courses, students’ scores on gradebook items like assignments and exams). Jayaprakash and Laur a (2014) Marist College Improve student retention rates in colleges. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. NA The large number of variables and their combinations made it difficult to analyse the suitable sequence of course-taking for differ- ent students. Advice on course selec- tion was given based on students’ scholastic characteristics and concurrent course enrolment. Students’ academic his- tory and demographic data, such as course enrolment information, course grade, GPA, ACT SAT sources, number of years taken to graduate and kind of degree obtained. 2014) A large American Mid- western university Determine the opti- mal term-by-term course selections for students. The one-on-one dialogue between the instructor and each student was not sustainable at scale. The analytics helped students to monitor and reflect on discus- sion participation, and encouraged dialogue between stu- dents and instructors. Analytics results were provided to learn- ers for framing their interpretation of online discussion as an integral course activity with clear goals and expectations. Log files and posts in the discussion forum (e.g. actions such as viewing creating edit- ing deleting posts, time and date, ID of users performing the actions, ID and length of posts being acted on). 2014) NA Provide pedagogical intervention based on students’ par- ticipation in online discussion. The email interventions had only a limited impact on student achievements. The interventions were associated with a higher final grade, especially for the students with a lower socio-economic status. Email messages were sent to students, sug- gesting online and in-person resources that could help them to improve their course performance. Students’ demographics (e.g. race ethnicity, socio-economic status and grade level) and learning activities on LMS (e.g. logins, exam and quiz grades and clicker points). 2015) San Diego State Uni- versity Identify at-risk students and take remedial actions. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Proactive remediation was provided by teach- ers on topics that stu- dents needed to learn but were not currently struggling with. Field observations of students collected dur- ing class (e.g. engaged disengaged behaviours using the learning system) and log data from the system. 2015) High schools in Texas and West Virginia Distinguish different types of student online and offline learning behav- iours to support teachers’ proactive remediation. The amount of teaching experience may affect teachers’ interpretation of data, and the amount and effectiveness of interventions. Teachers were enabled to attend to groups which needed help, and to address the problem issues. The topics, relevant concepts and progress revealed from the discussion were identi- fied and visualised. Teachers can give sug- gestions to deepen and broaden the discussion. Online chat messages of student groups, cover- ing the information such as topics, relevant concepts, number of words. 2015) NA Identify collabora- tive writing groups that experience problems and pro- vide suggestions. The effectiveness of the interventions was small for students at a low knowledge level. Students showed a bet- ter long-term reten- tion performance. Provision of personalised retention test schedules based on students’ knowledge levels, and relearning assignments if the students fail in the retention tests. Results of retention tests (based on the percentage of the number of questions answered correctly), and the speed to master relevant skills. Students showed a better learning performance after the interventions. Students’ affective states were detected, based on which instructive feedback, problem- solving feedback and reflective prompts were provided. Interaction with the learning platform (e.g. actions completed for learning tasks), students’ perceived task difficulty and the students’ spoken words (collected using speech recognition software). 2016) NA Provide tailored feedback to students based on their affective state. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Users’ socio-demo- graphic data, their learning goals, their activities within the learning environment and their organisation’s learning requirements. 2016b) Two European organi- sations Improve self- regulated learning through techno- logical scaffolding interventions. Self-regulated learning is affected by many factors and scaffolding is only one of them. The interventions were perceived as enhancing users’ self- regulated learning, in terms of areas such as recommending useful information, learning paths and activities. The intervention involved (i) Providing usage infor- mation about available resources (ii) Providing the latest updates on users’ learning goals and resources, as well as colleagues’ learning activities (iii) Showing users’ progress in achieving learning goals (iv) Recommending learning goals to col- leagues by the users (v) Informing users of the learning objectives and requirements of their organisation (vi) Recommending learning paths and learning activities to users (vii) Showing users’ profiles of knowledge sharing. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Students’ levels of engagement were enhanced in video watching, discussion and collaborative pro- gramming activities. Email messages were sent and face-to-face discussions were held with students. Log files of the edX LMS, which stored course informa- tion (e.g. number of students and course syllabus) and recorded students’ interactions with the LMS (e.g. video access and discussion posts) and students’ clickstream data collected from a web-based collabora- tive programming environment for meas- uring students’ levels of engagement. 2017) A university in Taiwan Improve students’ learning outcomes and level of engagement. More rigorous and objec- tive evaluation of the effectiveness of inter- ventions is necessary. There was a 7 higher pass rate in the exam for the students receiving interven- tions. Systematic interven- tions took place at various stages of a course, which included welcoming emails, follow-up phone calls, reminders for assign- ment quiz exam on LMS and face-to-face consultation. Students’ demographic data, prior academic results, assignment quiz scores in current courses, degree of interest in the course and in-class feedback collected using a stu- dent response system. 2018) A university in Hong Kong Identify at-risk students and implement proac- tive strategies. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. The long-term effective- ness of the inter- ventions providing formative feedback to students is not known. Students (especially those classified as high risk) benefitted from the interven- tion in terms of both achievement (i.e. getting a higher course grade) and persistence (i.e. course withdrawal). Students received mes- sages from instructors on their assessment results and feedback (e.g. encouragement and concern) on their level of performance. Scores on an assessment (e.g. quiz, assignment and exam) adminis- tered early in a term and recorded in the LMS. Espinoza and Genna (2018) University of El Paso Identify at-risk students and provide immediate feedback. Email messages were sent which outlined support options, tailored student support programme and teacher enabling course for developing skills for employment. 2018) University of New England Improve student retention Students’ demographic information (e.g. gender and age), insti- tutional variables (e.g. prior studies, study mode and course type), study performance and workload, daily emotion, online activi- ties, etc.. It may be difficult to determine how the results of this particular intervention can be generalised to other learning analytics practices. Students who used the dashboard more frequently tended to get higher course grades. Students’ performance in courses Feedback was provided to students via the dashboard (e.g. current performance, average performance of peers, estimate of students’ final grade and sug- gested resources). Provide feedback to students through dashboard as an intervention tool. Learning analytics refers to the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purposes of understanding and optimiz- ing learning and the environments in which it occurs (Siemens 2012). Given the massive amount of data available about learners and learning, it has been regarded as having great potential for offering learners a better learning experience that focuses on their individual preferences, strengths and needs (Clow 2013 Siemens 2012). 2017) state that, despite there being an increasing amount of research on small-scale, experimental implementation of intervention, there is not yet a comprehensive model supported by a strong evidence base for instructors to make effective interventions. The results offer insights into the formulation of intervention strategies for higher education institutions which practise learning analytics.. Literature review. Wise (2014) asserts that at-risk students may be weak at interpreting the learning analytics data and taking action according to them which requires strong metacogni- tive skills and self-regulation. Table 2 summarises the objectives of the learning analytics interventions. Table 1 Country region of the institutions practising learning analytics interventions. Seven cases did not specify the location of the institutions. Increase the effectiveness of tracking students’ learning process performance 2. Table 3 presents the data used to support the learning analytics interventions. Table 4 shows the intervention methods applied in the case studies. Enhanced productivity effectiveness in learning and teaching 4. Table 6 shows the challenges reported in the case studies regarding the intervention practices. Educ.. Table 5 Outcomes of the learning analytics interventions. Limited effectiveness for students at a low knowledge level 1. The findings also provide empirical evidence for researchers and practitioners about implementing learning analytics interventions. The interventions covered not only dealing with students encountering problems but also enhancing student success in general. 2017), but this has been progressively tackled, with institutions attempting to use creative ways for collecting the data which enable them to address the questions they are interested in. 2017), it is important to build up a data- informed culture so that academic decisions and early reactions on issues identified are based on empirical evidence. Source Institution Objective Data Intervention method Outcome Challenge. The tool enabled and facilitated the interpretation of each student’s activities and performance in the LMS. The student learning process was visualised for easy and intuitive interpretation by teachers. Assignments graded according to specific competency require- ments, expected course competencies and programme outcomes. Creepy analytics and learner data rights: Proceedings of the Ascilite 2014 Conference (pp. Sherpa: Increasing student success with a recommendation engine: Proceedings of the 2nd International Conference on Learning Analytics and Knowledge LAK’12 (pp. Cerezo, R., Suarez, N., N ez, J. C., S nchez-Santill n, M. (2014). eGraph tool: Graphing the learn- ing process in LMSs: Proceedings of the 4th International Conference on Learning Analytics and Knowledge LAK’14 (pp. Choi, S. P. M., Lam, S. S., Li, K. C., Wong, B. T. M. (2018). Learning analytics at low cost: At-risk student prediction with clicker data and systematic proactive interventions. The learning analytics cycle: Closing the loop effectively: Proceedings of the 2nd Inter- national Conference on Learning Analytics and Knowledge (pp. Corrin, L., Kennedy, G., de Barba, P. G., Lockyer, L., Ga evic, D., Williams, D., Bakharia, A. (2016). Completing the loop: Returning meaningful learning analytic data to teachers. Retrieved from http: melbo urne-cshe.unime lb.edu.au data asset s pdffi le 0006 20839 38 Loop Handb ook.pdf. Improving undergraduate student achievement in large blended courses through data-driven interventions: Proceedings of the Fifth International Confer- ence on Learning Analytics and Knowledge (pp. Hi, I want to talk to you about your progress: A large course intervention for at-risk college students. Journal of College Student Retention: Research, Theory Practice. Responsiveness-to-intervention: Definitions, evi- dence, and implications for the learning disabilities construct. How do we start State and directions of learning analyt- ics adoption. membe rclic ks.net asset s RESOU RCES draga n la repor t 20cc 20lic ence.pdf. Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success. Competency map: Visualizing student learning to promote student success: Proceedings of the 4th International Conference on Learning Analytics and Knowledge LAK’14 (pp. Grawemeyer, B., Mavrikis, M., Holmes, W., Gutierrez-Santos, S., Wiedmann, M., Rummel, N. (2016). Affecting off-task behaviour: How affect-aware feedback can improve student learning: Proceedings of the 5th International Conference on Learning Analytics and Knowledge LAK’16 (pp. Open academic early alert system: Technical demon- stration: Proceedings of the 4th International Conference on Learning Analytics and Knowledge LAK’14 (pp. Learning analytics: Principles and constraints: Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications 2015 (pp. Course signals at Purdue: Using learning analytics to increase student success. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge LAK’12 (pp. Status of learning analytics in Asia: Perspectives of higher education stakeholders. Technology in education: Innovative solutions and practices (pp. Issues, challenges, and lessons learned when scaling up a learning analytics intervention: Proceedings of the 3rd International Conference on Learning Ana- lytics and Knowledge LAK’13 (pp.\n"
          ]
        }
      ],
      "source": [
        "summary = bert(text)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": false,
        "id": "2Zdio-tYooIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9deef4b-f6bf-4afd-e17e-411e0dfe6a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['intervention involved', 'institution ’', 'need ..', 'intervention method', 'commonly exist', 'learning behaviours', 'students ’', 'students via', 'intervention methods', 'particular intervention', 'students may', 'study progress', 'student learning', 'higher education', 'student online', 'student problems', 'learning analytics', 'helping students', 'study information', 'learning data', 'outcome', 'dent demographic', 'online learning', 'learning problems', 'collaborative learning', 'interventions covered', 'intervention strategies', 'task behaviour', 'faculty ’', 'learning progress']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "sentences = sent_tokenize(text)\n",
        "sentences = list(filter(lambda x: len(x.split()) > 7, sentences))\n",
        "sentences = [re.sub('\\n', '', i) for i in sentences]\n",
        "text = \" \".join(sentences)\n",
        "keyword = get_keyword(summary, text)\n",
        "print(keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": false,
        "id": "UKI9ANy7ooIO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "e67c0ecd-05f7-4142-bd84-f778db10fd2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sentence               key_phrase\n",
              "0   One major objective of learning analytics is t...       learning analytics\n",
              "1   Although the idea of making learning analytics...  intervention strategies\n",
              "2   This paper aims to review the case studies of ...         higher education\n",
              "3   The cases were categorised and summarised acco...                  outcome\n",
              "4   The frequent types of data involved students’ ...      learning behaviours\n",
              "5   Within the limited availability of empirical s...      intervention method\n",
              "6   Educ.. are informed by data on the learning pr...        learning progress\n",
              "7   Wise (2014) asserts that at-risk students may ...             students may\n",
              "8   The frequent types of data involved students’ ...          online learning\n",
              "9   Within the limited availability of empirical s...     intervention methods\n",
              "10  The most commonly used intervention methods in...            learning data\n",
              "11  There were also a few types of data which were...           study progress\n",
              "12  Other types of objectives focused on the stude...         helping students\n",
              "13  Other types of objectives focused on the stude...   collaborative learning\n",
              "14  Visuali- sation of learning data was the secon...         student learning\n",
              "15  The interventions covered not only dealing wit...    interventions covered\n",
              "16  Rather than early detection or prediction of s...        learning problems\n",
              "17  To allow learning analytics inter- vention to ...           commonly exist\n",
              "18  (2018) for using different intervention method...         student problems\n",
              "19  Personalised recommen- dations were provided c...        study information\n",
              "20  Grade information, student demographic and cou...         dent demographic\n",
              "21  (2015) High schools in Texas and West Virginia...           student online\n",
              "22  The intervention involved (i) Providing usage ...    intervention involved\n",
              "23  It may be difficult to determine how the resul...  particular intervention\n",
              "24  Students’ performance in courses Feedback was ...             students via\n",
              "25  Affecting off-task behaviour: How affect-aware...           task behaviour"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88e17cb3-dc13-4700-8a4b-aa282fd6c4ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>key_phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One major objective of learning analytics is t...</td>\n",
              "      <td>learning analytics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Although the idea of making learning analytics...</td>\n",
              "      <td>intervention strategies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This paper aims to review the case studies of ...</td>\n",
              "      <td>higher education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The cases were categorised and summarised acco...</td>\n",
              "      <td>outcome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The frequent types of data involved students’ ...</td>\n",
              "      <td>learning behaviours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Within the limited availability of empirical s...</td>\n",
              "      <td>intervention method</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Educ.. are informed by data on the learning pr...</td>\n",
              "      <td>learning progress</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Wise (2014) asserts that at-risk students may ...</td>\n",
              "      <td>students may</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The frequent types of data involved students’ ...</td>\n",
              "      <td>online learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Within the limited availability of empirical s...</td>\n",
              "      <td>intervention methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>The most commonly used intervention methods in...</td>\n",
              "      <td>learning data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>There were also a few types of data which were...</td>\n",
              "      <td>study progress</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Other types of objectives focused on the stude...</td>\n",
              "      <td>helping students</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Other types of objectives focused on the stude...</td>\n",
              "      <td>collaborative learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Visuali- sation of learning data was the secon...</td>\n",
              "      <td>student learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>The interventions covered not only dealing wit...</td>\n",
              "      <td>interventions covered</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Rather than early detection or prediction of s...</td>\n",
              "      <td>learning problems</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>To allow learning analytics inter- vention to ...</td>\n",
              "      <td>commonly exist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(2018) for using different intervention method...</td>\n",
              "      <td>student problems</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Personalised recommen- dations were provided c...</td>\n",
              "      <td>study information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Grade information, student demographic and cou...</td>\n",
              "      <td>dent demographic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(2015) High schools in Texas and West Virginia...</td>\n",
              "      <td>student online</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>The intervention involved (i) Providing usage ...</td>\n",
              "      <td>intervention involved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>It may be difficult to determine how the resul...</td>\n",
              "      <td>particular intervention</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Students’ performance in courses Feedback was ...</td>\n",
              "      <td>students via</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Affecting off-task behaviour: How affect-aware...</td>\n",
              "      <td>task behaviour</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88e17cb3-dc13-4700-8a4b-aa282fd6c4ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88e17cb3-dc13-4700-8a4b-aa282fd6c4ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88e17cb3-dc13-4700-8a4b-aa282fd6c4ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "ks_list = pd.DataFrame(columns = ['sentence', 'key_phrase'])\n",
        "for s in sentences:\n",
        "    for w in keyword:\n",
        "        if w in s:\n",
        "            dic = {'sentence':s,'key_phrase':w}\n",
        "            ks_list = ks_list.append(dic, ignore_index = True)\n",
        "            keyword.remove(w)\n",
        "            sentences.remove(s)\n",
        "            break\n",
        "ks_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "id": "lABYckhUooIP"
      },
      "outputs": [],
      "source": [
        "ks_list.to_csv(filename.split('_')[0] +\"_cloze.csv\",encoding='utf-8-sig',index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c06ae7238053412abfd8a1d417e64935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e4b69ea0c964010ad44d3fd2b448cd2",
              "IPY_MODEL_e2f479c3e8d74eb98512c4041935144d",
              "IPY_MODEL_9c3011730b814f3294bff5e5114d5ddd"
            ],
            "layout": "IPY_MODEL_d758c6801be74a46bd2386c3f45e2e1b"
          }
        },
        "1e4b69ea0c964010ad44d3fd2b448cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_302014b078d24e94844b50d68bd0bf98",
            "placeholder": "​",
            "style": "IPY_MODEL_b028527927cf422b94c1c7a0235194dd",
            "value": ""
          }
        },
        "e2f479c3e8d74eb98512c4041935144d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6828169acb854a05822facb5660a90f4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15d842bb7dae441d8837208908669dea",
            "value": 0
          }
        },
        "9c3011730b814f3294bff5e5114d5ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77e5ba96d0904453965850a53b270bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_1991239b493d4c4eb1a3fa99179c226b",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "d758c6801be74a46bd2386c3f45e2e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "302014b078d24e94844b50d68bd0bf98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b028527927cf422b94c1c7a0235194dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6828169acb854a05822facb5660a90f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "15d842bb7dae441d8837208908669dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77e5ba96d0904453965850a53b270bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1991239b493d4c4eb1a3fa99179c226b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dede63bfa8840f48bc2f1dcc4e2b337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e6caa652c7b44d2ac0d92a91f4be00e",
              "IPY_MODEL_faed98f719674c5f896c1f09c64d0c34",
              "IPY_MODEL_ba2fd43d0a2a4341a7aa8df4214f67ee"
            ],
            "layout": "IPY_MODEL_178c814fa2c74992837dd3c05c773df5"
          }
        },
        "3e6caa652c7b44d2ac0d92a91f4be00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb5d0d68094648efa6d394c9c23d429c",
            "placeholder": "​",
            "style": "IPY_MODEL_fc1f98b368774a7c83fbf9fcf5dacc4b",
            "value": "Downloading: 100%"
          }
        },
        "faed98f719674c5f896c1f09c64d0c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37984fcb312493d8b0c6ff5cd5ecdcf",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c71ab8470d86488b9bfc097195247a91",
            "value": 571
          }
        },
        "ba2fd43d0a2a4341a7aa8df4214f67ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb006a0df994b45b4e28fac6d0af903",
            "placeholder": "​",
            "style": "IPY_MODEL_124c0f85adb14a6b85acc9a965d2f2b9",
            "value": " 571/571 [00:00&lt;00:00, 15.3kB/s]"
          }
        },
        "178c814fa2c74992837dd3c05c773df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb5d0d68094648efa6d394c9c23d429c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc1f98b368774a7c83fbf9fcf5dacc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d37984fcb312493d8b0c6ff5cd5ecdcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71ab8470d86488b9bfc097195247a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdb006a0df994b45b4e28fac6d0af903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "124c0f85adb14a6b85acc9a965d2f2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6bec2e38f6247f5a1f9e0a46d3f2fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e35c9cec20c448ae9e9622e4dfa9e635",
              "IPY_MODEL_d6ec2d2370ec4143a9a07708c95ade68",
              "IPY_MODEL_e290dfeef01b42928c45303215149ca5"
            ],
            "layout": "IPY_MODEL_c7868ca9b66c40f2a091db6db705b41b"
          }
        },
        "e35c9cec20c448ae9e9622e4dfa9e635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01aade197295492fa7f8533bcdcf85c6",
            "placeholder": "​",
            "style": "IPY_MODEL_987fbc078dee4aefaf4e7f31ddff93a2",
            "value": "Downloading: 100%"
          }
        },
        "d6ec2d2370ec4143a9a07708c95ade68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30b870f983704221921269a9c6170bce",
            "max": 1344997306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0f540538d5b48bc98f01437b8154fc8",
            "value": 1344997306
          }
        },
        "e290dfeef01b42928c45303215149ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f72900689c14b4b8fb2369b4ae6dc9b",
            "placeholder": "​",
            "style": "IPY_MODEL_906cd4aae8cf4f00bd2baac8ff24c0fc",
            "value": " 1.34G/1.34G [00:30&lt;00:00, 33.7MB/s]"
          }
        },
        "c7868ca9b66c40f2a091db6db705b41b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01aade197295492fa7f8533bcdcf85c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987fbc078dee4aefaf4e7f31ddff93a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30b870f983704221921269a9c6170bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0f540538d5b48bc98f01437b8154fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f72900689c14b4b8fb2369b4ae6dc9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "906cd4aae8cf4f00bd2baac8ff24c0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf11426f0de5464880b3151f04d052db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ca245fae6954482b1dacddde060388f",
              "IPY_MODEL_8e88fa1181fd405dae9299bd78d34225",
              "IPY_MODEL_82ec2b5a84f347af9a322c39ae13b13e"
            ],
            "layout": "IPY_MODEL_58de8685cb5d4bd4b7e7bc57ca74fb5f"
          }
        },
        "4ca245fae6954482b1dacddde060388f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df8cb36dc3f4132b9007b4e583a6545",
            "placeholder": "​",
            "style": "IPY_MODEL_48f990e5df9a47c4887fd7b9fcd25cc0",
            "value": "Downloading: 100%"
          }
        },
        "8e88fa1181fd405dae9299bd78d34225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ced279e45b4464a159d0890ddf64ef",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4f7ee4965e54df2a94ce40c172ae36f",
            "value": 231508
          }
        },
        "82ec2b5a84f347af9a322c39ae13b13e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2648beb1459e47a4b4f061b230c7221c",
            "placeholder": "​",
            "style": "IPY_MODEL_afe472a57f364c85bec98539f81cc022",
            "value": " 232k/232k [00:00&lt;00:00, 972kB/s]"
          }
        },
        "58de8685cb5d4bd4b7e7bc57ca74fb5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df8cb36dc3f4132b9007b4e583a6545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f990e5df9a47c4887fd7b9fcd25cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9ced279e45b4464a159d0890ddf64ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f7ee4965e54df2a94ce40c172ae36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2648beb1459e47a4b4f061b230c7221c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe472a57f364c85bec98539f81cc022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5d3b7ddb3042978603a821d65c3098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d85cb345b07449bbe1b31b0d815dfb4",
              "IPY_MODEL_7890e1828ae047609599334d0dad7186",
              "IPY_MODEL_9bf215929dbe4fbbab3eb377d28cba80"
            ],
            "layout": "IPY_MODEL_e59b89da4a364e818ebb614eec15be3a"
          }
        },
        "9d85cb345b07449bbe1b31b0d815dfb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112e540969c1489e88cc05ad57d7a072",
            "placeholder": "​",
            "style": "IPY_MODEL_5f52427017784b45b94c1ca508823538",
            "value": "Downloading: 100%"
          }
        },
        "7890e1828ae047609599334d0dad7186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f8fe63eb5248f8bdfa8f165fb1b1c2",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_715632eeb59b4e038e7b6fb8354f52ad",
            "value": 28
          }
        },
        "9bf215929dbe4fbbab3eb377d28cba80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fac1809797b4ce0aeae44c9db9445c0",
            "placeholder": "​",
            "style": "IPY_MODEL_0c17f4eda968422cba165cfeebf13a74",
            "value": " 28.0/28.0 [00:00&lt;00:00, 252B/s]"
          }
        },
        "e59b89da4a364e818ebb614eec15be3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112e540969c1489e88cc05ad57d7a072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f52427017784b45b94c1ca508823538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4f8fe63eb5248f8bdfa8f165fb1b1c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715632eeb59b4e038e7b6fb8354f52ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fac1809797b4ce0aeae44c9db9445c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c17f4eda968422cba165cfeebf13a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}